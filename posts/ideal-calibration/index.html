<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=0.9">
    <title>Ideal theory as calibration—ColEx</title>
    <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin>
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,300,700,400italic" rel="stylesheet" type="text/css">
    <link rel="stylesheet" type="text/css" href="../../css/default.css" />
    <link rel="icon" href="../../images/favicon.ico" type="image/x-icon" />
    <link rel="shortcut icon" href="../../images/favicon.ico" type="image/x-icon" />
    <script>
     if (!location.host.startsWith('localhost')) {
       var _rollbarConfig = {
         checkIgnore: function(isUncaught, args, payload) {
           return location.host.startsWith('localhost')
         },
         accessToken: "137e3ab64049469ba4a7d5e13a6f5aeb",
         captureUncaught: true,
         payload: {
           environment: "production"
         }
       };
       !function(r){function o(n){if(e[n])return e[n].exports;var t=e[n]={exports:{},id:n,loaded:!1};return r[n].call(t.exports,t,t.exports,o),t.loaded=!0,t.exports}var e={};return o.m=r,o.c=e,o.p="",o(0)}([function(r,o,e){"use strict";var n=e(1),t=e(4);_rollbarConfig=_rollbarConfig||{},_rollbarConfig.rollbarJsUrl=_rollbarConfig.rollbarJsUrl||"https://cdnjs.cloudflare.com/ajax/libs/rollbar.js/2.3.1/rollbar.min.js",_rollbarConfig.async=void 0===_rollbarConfig.async||_rollbarConfig.async;var a=n.setupShim(window,_rollbarConfig),l=t(_rollbarConfig);window.rollbar=n.Rollbar,a.loadFull(window,document,!_rollbarConfig.async,_rollbarConfig,l)},function(r,o,e){"use strict";function n(r){return function(){try{return r.apply(this,arguments)}catch(r){try{console.error("[Rollbar]: Internal error",r)}catch(r){}}}}function t(r,o){this.options=r,this._rollbarOldOnError=null;var e=s++;this.shimId=function(){return e},window&&window._rollbarShims&&(window._rollbarShims[e]={handler:o,messages:[]})}function a(r,o){var e=o.globalAlias||"Rollbar";if("object"==typeof r[e])return r[e];r._rollbarShims={},r._rollbarWrappedError=null;var t=new p(o);return n(function(){o.captureUncaught&&(t._rollbarOldOnError=r.onerror,i.captureUncaughtExceptions(r,t,!0),i.wrapGlobals(r,t,!0)),o.captureUnhandledRejections&&i.captureUnhandledRejections(r,t,!0);var n=o.autoInstrument;return(void 0===n||n===!0||"object"==typeof n&&n.network)&&r.addEventListener&&(r.addEventListener("load",t.captureLoad.bind(t)),r.addEventListener("DOMContentLoaded",t.captureDomContentLoaded.bind(t))),r[e]=t,t})()}function l(r){return n(function(){var o=this,e=Array.prototype.slice.call(arguments,0),n={shim:o,method:r,args:e,ts:new Date};window._rollbarShims[this.shimId()].messages.push(n)})}var i=e(2),s=0,d=e(3),c=function(r,o){return new t(r,o)},p=d.bind(null,c);t.prototype.loadFull=function(r,o,e,t,a){var l=function(){var o;if(void 0===r._rollbarDidLoad){o=new Error("rollbar.js did not load");for(var e,n,t,l,i=0;e=r._rollbarShims[i++];)for(e=e.messages||[];n=e.shift();)for(t=n.args||[],i=0;i<t.length;++i)if(l=t[i],"function"==typeof l){l(o);break}}"function"==typeof a&&a(o)},i=!1,s=o.createElement("script"),d=o.getElementsByTagName("script")[0],c=d.parentNode;s.crossOrigin="",s.src=t.rollbarJsUrl,e||(s.async=!0),s.onload=s.onreadystatechange=n(function(){if(!(i||this.readyState&&"loaded"!==this.readyState&&"complete"!==this.readyState)){s.onload=s.onreadystatechange=null;try{c.removeChild(s)}catch(r){}i=!0,l()}}),c.insertBefore(s,d)},t.prototype.wrap=function(r,o,e){try{var n;if(n="function"==typeof o?o:function(){return o||{}},"function"!=typeof r)return r;if(r._isWrap)return r;if(!r._rollbar_wrapped&&(r._rollbar_wrapped=function(){e&&"function"==typeof e&&e.apply(this,arguments);try{return r.apply(this,arguments)}catch(e){var o=e;throw"string"==typeof o&&(o=new String(o)),o._rollbarContext=n()||{},o._rollbarContext._wrappedSource=r.toString(),window._rollbarWrappedError=o,o}},r._rollbar_wrapped._isWrap=!0,r.hasOwnProperty))for(var t in r)r.hasOwnProperty(t)&&(r._rollbar_wrapped[t]=r[t]);return r._rollbar_wrapped}catch(o){return r}};for(var u="log,debug,info,warn,warning,error,critical,global,configure,handleUncaughtException,handleUnhandledRejection,captureDomContentLoaded,captureLoad".split(","),f=0;f<u.length;++f)t.prototype[u[f]]=l(u[f]);r.exports={setupShim:a,Rollbar:p}},function(r,o){"use strict";function e(r,o,e){if(r){var t;"function"==typeof o._rollbarOldOnError?t=o._rollbarOldOnError:r.onerror&&!r.onerror.belongsToShim&&(t=r.onerror,o._rollbarOldOnError=t);var a=function(){var e=Array.prototype.slice.call(arguments,0);n(r,o,t,e)};a.belongsToShim=e,r.onerror=a}}function n(r,o,e,n){r._rollbarWrappedError&&(n[4]||(n[4]=r._rollbarWrappedError),n[5]||(n[5]=r._rollbarWrappedError._rollbarContext),r._rollbarWrappedError=null),o.handleUncaughtException.apply(o,n),e&&e.apply(r,n)}function t(r,o,e){if(r){"function"==typeof r._rollbarURH&&r._rollbarURH.belongsToShim&&r.removeEventListener("unhandledrejection",r._rollbarURH);var n=function(r){var e=r.reason,n=r.promise,t=r.detail;!e&&t&&(e=t.reason,n=t.promise),o&&o.handleUnhandledRejection&&o.handleUnhandledRejection(e,n)};n.belongsToShim=e,r._rollbarURH=n,r.addEventListener("unhandledrejection",n)}}function a(r,o,e){if(r){var n,t,a="EventTarget,Window,Node,ApplicationCache,AudioTrackList,ChannelMergerNode,CryptoOperation,EventSource,FileReader,HTMLUnknownElement,IDBDatabase,IDBRequest,IDBTransaction,KeyOperation,MediaController,MessagePort,ModalWindow,Notification,SVGElementInstance,Screen,TextTrack,TextTrackCue,TextTrackList,WebSocket,WebSocketWorker,Worker,XMLHttpRequest,XMLHttpRequestEventTarget,XMLHttpRequestUpload".split(",");for(n=0;n<a.length;++n)t=a[n],r[t]&&r[t].prototype&&l(o,r[t].prototype,e)}}function l(r,o,e){if(o.hasOwnProperty&&o.hasOwnProperty("addEventListener")){for(var n=o.addEventListener;n._rollbarOldAdd&&n.belongsToShim;)n=n._rollbarOldAdd;var t=function(o,e,t){n.call(this,o,r.wrap(e),t)};t._rollbarOldAdd=n,t.belongsToShim=e,o.addEventListener=t;for(var a=o.removeEventListener;a._rollbarOldRemove&&a.belongsToShim;)a=a._rollbarOldRemove;var l=function(r,o,e){a.call(this,r,o&&o._rollbar_wrapped||o,e)};l._rollbarOldRemove=a,l.belongsToShim=e,o.removeEventListener=l}}r.exports={captureUncaughtExceptions:e,captureUnhandledRejections:t,wrapGlobals:a}},function(r,o){"use strict";function e(r,o){this.impl=r(o,this),this.options=o,n(e.prototype)}function n(r){for(var o=function(r){return function(){var o=Array.prototype.slice.call(arguments,0);if(this.impl[r])return this.impl[r].apply(this.impl,o)}},e="log,debug,info,warn,warning,error,critical,global,configure,handleUncaughtException,handleUnhandledRejection,_createItem,wrap,loadFull,shimId,captureDomContentLoaded,captureLoad".split(","),n=0;n<e.length;n++)r[e[n]]=o(e[n])}e.prototype._swapAndProcessMessages=function(r,o){this.impl=r(this.options);for(var e,n,t;e=o.shift();)n=e.method,t=e.args,this[n]&&"function"==typeof this[n]&&("captureDomContentLoaded"===n||"captureLoad"===n?this[n].apply(this,[t[0],e.ts]):this[n].apply(this,t));return this},r.exports=e},function(r,o){"use strict";r.exports=function(r){return function(o){if(!o&&!window._rollbarInitialized){r=r||{};for(var e,n,t=r.globalAlias||"Rollbar",a=window.rollbar,l=function(r){return new a(r)},i=0;e=window._rollbarShims[i++];)n||(n=e.handler),e.handler._swapAndProcessMessages(l,e.messages);window[t]=n,window._rollbarInitialized=!0}}}}]);
     }
    </script>
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-113913768-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-113913768-1');
    </script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          displayAlign: "left",
          displayIndent: "2em"
        });
    </script>
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_SVG"></script>

    
  <link rel="stylesheet" type="text/css" href="../../css/ideal-calibration.css" />


  <meta name="description" content="<p>The ideal serves not only as destination but as calibration. Once we acknowledge our ignorance of possible worlds, we must treat the task of social engineering as a problem of statistical inference. From the statistical inference perspective, the ideal (maximum) is very informative about the underlying distribution of possible worlds and helps us make more informed trade-offs.</p>">

</head>
<body>
<div id="underlay">
<nav><a href="../../">Home</a></nav>
<main><article>
  <h2 id="article-title">Ideal theory as calibration</h2>
  
  <div class="metadata">
  <nav class="tags"><ul>
    
      <li><a href="../../tag/ideal%2520theory/">ideal theory</a></li>
    
      <li><a href="../../tag/political%2520philosophy/">political philosophy</a></li>
    
</ul></nav>

  
    <nav><a class="series" href="../../series/The%2520Tyranny%2520of%2520the%2520Ideal/">Series: The Tyranny of the Ideal</a></nav>
  
  <span class="date">Published on <time datetime="2018-06-20">June 20, 2018</time>.</span>
  
  </div>
  
  
    <div class="abstract">
      <p>The ideal serves not only as destination but as calibration. Once we acknowledge our ignorance of possible worlds, we must treat the task of social engineering as a problem of statistical inference. From the statistical inference perspective, the ideal (maximum) is very informative about the underlying distribution of possible worlds and helps us make more informed trade-offs.</p>
    </div>
  

  
    <aside class="sidenote" id="warnings">Warnings: <ul>
      
        <li><details><summary>Belaborious</summary><p>Arguably belabors the point. If you think you understand the point, you probably do.</p></details></li>
      
    </ul></aside>
  

  <div class="macros">

</div>
<h3 id="intro">Intro</h3>
<p><a href="../../posts/utopia-infinitude-secretaries/">Last time</a>, I described how <span class="citation" data-cites="gaus2016">(Gaus <a href="#ref-gaus2016">2016</a>)</span> juxtaposes unidimensional and multidimensional models of justice. I went on to contest the claim that the ideal is otiose in the unidimensional model and made an analogy to <a href="https://en.wikipedia.org/wiki/Secretary_problem">the secretary problem</a>.</p>
<p>This time I’ll make the (related) argument directly that there are two distinct uses of ideal theorizing and only one is bad from the unidimensional perspective.</p>
<h3 id="dimensionality-of-justice">Dimensionality of justice</h3>
<p>Let’s try to formalize ‘unidimensional’ and ‘multidimensional’ models of justice so we can be sure we’re thinking of the same thing. Gaus suggests (and I’ll accept) that a key part of any theory of ideal justice includes a function <span class="noted"><span class="math inline">\(e \colon \mathbb{W} \to \mathbb{R}\)</span></span><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>, a set of possible worlds <span class="math inline">\(\mathbb{W}\)</span>. In other words, each such theory should be able to assign a ‘justice score’ to every possible world. In terms of this machinery, the unidimensional model simply limits itself to using <em>only</em> <span class="math inline">\(e\)</span> and <span class="math inline">\(\mathbb{W}\)</span>. The multidimensional model on the other hand also gives us a tool to inspect the structure of <span class="math inline">\(\mathbb{W}\)</span> in the form of a <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a> <span class="math inline">\(d \colon \mathbb{W} \times \mathbb{W} \to [0,\inf)\)</span>. In other words, the multidimensional model lets us determine how similar two possible worlds are in some way that’s not directly related to their justice scores.</p>
<p>To actually use this in a model, we’ll also need a way of finding worlds <span class="math inline">\(W\)</span> from <span class="math inline">\(\mathbb{W}\)</span> to evaluate. We denote a a random, ‘nearby’ (i.e. one with a small distance <span class="math inline">\(d(W, W_c)\)</span> from the then-current world <span class="math inline">\(W_c\)</span>) world as <span class="math inline">\(W_r\)</span>.</p>
<h3 id="ideal-as-destination">Ideal as destination</h3>
<p><span class="citation" data-cites="gaus2016">(Gaus <a href="#ref-gaus2016">2016</a>)</span> and the rest of the literature suggest that the an ideal is useful as a destination. According to Rawls, “By showing how the social world may realize the features of a realistic Utopia, political philosophy provides a long-term goal of political endeavor, and in working toward it gives meaning to what we can do today.” <span class="citation" data-cites="rawls1993">(Rawls <a href="#ref-rawls1993">1993</a>)</span></p>
<p>In terms of our model, the ideal is <span class="math inline">\(\mathop{\mathrm{argmax}}\limits_{W \in \mathbb{W}} e(W) = W_i\)</span>, the possible world that achieves the highest justice score. A wholly naive algorithm would then:</p>
<ol type="1">
<li>Use our ideal <span class="math inline">\(W_i\)</span> to orient societal progress by always picking <span class="math inline">\(\mathop{\mathrm{argmin}}\limits_{W \in \{W_c, W_r\}} d(W, W_i) = W_{bk}\)</span>. In other words, on every ‘step’, someone proposes some random alternative world <span class="math inline">\(W_r\)</span> and this naive algorithm compares it to the current world and picks whichever is closer to the ideal.</li>
<li>Stop when <span class="math inline">\(d(W_c, W_i) = 0\)</span>.</li>
</ol>
<p>With this interpretation, it’s clear that the ideal as destination as not only otiose in the unidimensional model but nonsensical. The unidimensional perspective was defined by its omission of the metric <span class="math inline">\(d\)</span> so we certainly can’t use it in our algorithm to find <span class="math inline">\(W_{bk}\)</span>.</p>
<!--more-->
<h3 id="ideal-as-calibration">Ideal as calibration</h3>
<p>But, as we suggested in the previous post, this is not the only role that an ideal can play. In real life, we are not so perfectly informed as the simple mathematical model suggests. We can model our ignorance as some combination of:</p>
<ul>
<li>Knowledge only of some proper subset of possible worlds: <span class="math inline">\(\mathbb{W}_k \subset \mathbb{W}\)</span></li>
<li>An inability to evaluate the justice of some alternative worlds. That is, our function is partial: <span class="math inline">\(e_p \colon \mathbb{W} ⇸ \mathbb{R}\)</span></li>
</ul>
<p>The best world we know of is then <span class="math inline">\(\mathop{\mathrm{argmax}}\limits_{W \in \mathbb{W}_k} e_p(W) = W_{bk}\)</span>. Also, there is some cost to exploring possible worlds—if nothing else, the opportunity cost of living in worse worlds.</p>
<h4 id="unidimensional">Unidimensional</h4>
<p>With this understanding in place, we can construct a naive algorithm just like the earlier one we described for the unidimensional case.</p>
<ol type="1">
<li>Use our ideal <span class="math inline">\(W_i\)</span> to orient societal progress by always picking <span class="math inline">\(\mathop{\mathrm{argmax}}\limits{W \in \{W_c, W_r\}} e(W) = W_{bk}\)</span>.</li>
<li>Stop when <span class="math inline">\(e(W_c) = e(W_i)\)</span>.</li>
</ol>
<p>We see that knowledge of the ideal is crucial in our stopping condition. Without it, it would be hard to determine when to stop the search and stop incurring search costs.</p>
<p>But we could still make a best guess about whether to stay or go, even without knowledge of the ideal. We can treat it as a problem of <a href="https://en.wikipedia.org/wiki/Statistical_inference">statistical inference</a>. We have a sample of possible worlds (the worlds we’ve experienced) and would like to estimate the true distribution of possible worlds. Once we have an estimate as to the distribution of possible worlds, we can make a more informed decision on whether to continue striving or to accept the status quo.</p>
<p>For example, if we’ve experienced many mediocre worlds in the past and only a few that are nearly as good as <span class="math inline">\(W_{bk}\)</span> <span id="in-upper-spark" class="spark"></span> (and if we, for the sake of expository convenience, suppose that possible worlds are normally distributed with respect to justice score), we should suspect that we’re in the upper tail of the distribution and be correspondingly cautious. Contrariwise, if we’ve experienced a few bad worlds in the past and many worlds about as good as ours <span id="in-middle-spark" class="spark"></span>, we should suspect that we’re in the fattest part of the distribution and that there’s still substantial room for improvement.</p>
<p>So even though the ideal isn’t absolutely necessary to provide a stopping condition in the unidimensional case, it is useful. Extrema have an outsize influence on our estimate of the distribution of possible worlds. If we’ve only experienced a handful of mediocre worlds <span id="mediocre-spark" class="spark"></span>, we might suppose that distribution of possible worlds is mediocre. If however, we learn of an ideal world that is far, far better <span id="ideal-spark" class="spark"></span>, our estimate of the distribution changes and so our actions should change accordingly.</p>
<h4 id="multidimensional">Multidimensional</h4>
<p>We can even transport this calibration logic back to the multidimensional perspective. The most interesting task in this setting is demonstrating that ideal as calibration is distinct from ideal as destination: First, suppose we lack an ideal theory and currently occupy a local maximum. If we’re sufficiently uncertain about the underlying distribution of possible worlds, our social engineering algorithm may well suggest we abandon this peak in search of higher peaks. We may even eventually end up at the true ideal (global maximum). Now, imagine that we <em>do</em> have an ideal theory, but it says that the ideal is only the teensiest smidge better than our current local maximum. Because traversing the landscape of possible worlds is costly, our algorithm almost certainly suggests that we stay at our current peak indefinitely. In other words, knowledge of the ideal has <em>discouraged</em> us from pursuing it. It has dictated that we stay where we are rather than pursue the ideal. This is impossible to explain if the ideal <em>only</em> has value as a destination.</p>
<h3 id="conclusion">Conclusion</h3>
<p>The ideal can indeed serve as destination (i.e. we continually move toward the ideal). But it can also serve as calibration. Once we acknowledge our ignorance of possible worlds, we must treat the task of social engineering as a problem of statistical inference. From the statistical inference perspective, the ideal (maximum) is very informative about the underlying distribution of possible worlds and helps us make more informed trade-offs.</p>
<hr class="references">
<div id="refs" class="references">
<div id="ref-gaus2016">
<p>Gaus, Gerald. 2016. <em>The Tyranny of the Ideal: Justice in a Diverse Society</em>. Princeton University Press.</p>
</div>
<div id="ref-rawls1993">
<p>Rawls, John. 1993. “The Law of Peoples.” <em>Critical Inquiry</em> 20 (1). University of Chicago Press: 36–68.</p>
</div>
</div>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>I think Gaus actually jumps to the conclusion of a cardinal evaluation function (rather than ordinal) too quickly, but we’ll set that aside for the moment.<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</section>
</article></main>

</div>




<script defer type="text/javascript" src="../../js/custom-elements.js"></script>

<script defer type="text/javascript" src="../../js/vendors~arg-map~custom-elements~util-egal.js"></script>


<script defer type="text/javascript" src="../../js/ideal-calibration.js"></script>

<script defer type="text/javascript" src="../../js/vendors~false-dichotomy-ideal-theory-debate~ideal-calibration~is-development-easy~quorum~util-egal.js"></script>

<script defer type="text/javascript" src="../../js/vendors~false-dichotomy-ideal-theory-debate~ideal-calibration~is-development-easy~quorum.js"></script>


  </body>
</html>
