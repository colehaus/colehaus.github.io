<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=0.9">
    <title>Uncertainty and sensitivity analysis of GiveWell's top charity rankings—ColEx</title>
    <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin>
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,300,700,400italic" rel="stylesheet" type="text/css">
    <link rel="stylesheet" type="text/css" href="../../css/default.css" />
    <link rel="icon" href="../../images/favicon.ico" type="image/x-icon" />
    <link rel="shortcut icon" href="../../images/favicon.ico" type="image/x-icon" />
    <script>
     if (!location.host.startsWith('localhost')) {
       var _rollbarConfig = {
         checkIgnore: function(isUncaught, args, payload) {
           return location.host.startsWith('localhost')
         },
         accessToken: "137e3ab64049469ba4a7d5e13a6f5aeb",
         captureUncaught: true,
         payload: {
           environment: "production"
         }
       };
       !function(r){function o(n){if(e[n])return e[n].exports;var t=e[n]={exports:{},id:n,loaded:!1};return r[n].call(t.exports,t,t.exports,o),t.loaded=!0,t.exports}var e={};return o.m=r,o.c=e,o.p="",o(0)}([function(r,o,e){"use strict";var n=e(1),t=e(4);_rollbarConfig=_rollbarConfig||{},_rollbarConfig.rollbarJsUrl=_rollbarConfig.rollbarJsUrl||"https://cdnjs.cloudflare.com/ajax/libs/rollbar.js/2.3.1/rollbar.min.js",_rollbarConfig.async=void 0===_rollbarConfig.async||_rollbarConfig.async;var a=n.setupShim(window,_rollbarConfig),l=t(_rollbarConfig);window.rollbar=n.Rollbar,a.loadFull(window,document,!_rollbarConfig.async,_rollbarConfig,l)},function(r,o,e){"use strict";function n(r){return function(){try{return r.apply(this,arguments)}catch(r){try{console.error("[Rollbar]: Internal error",r)}catch(r){}}}}function t(r,o){this.options=r,this._rollbarOldOnError=null;var e=s++;this.shimId=function(){return e},window&&window._rollbarShims&&(window._rollbarShims[e]={handler:o,messages:[]})}function a(r,o){var e=o.globalAlias||"Rollbar";if("object"==typeof r[e])return r[e];r._rollbarShims={},r._rollbarWrappedError=null;var t=new p(o);return n(function(){o.captureUncaught&&(t._rollbarOldOnError=r.onerror,i.captureUncaughtExceptions(r,t,!0),i.wrapGlobals(r,t,!0)),o.captureUnhandledRejections&&i.captureUnhandledRejections(r,t,!0);var n=o.autoInstrument;return(void 0===n||n===!0||"object"==typeof n&&n.network)&&r.addEventListener&&(r.addEventListener("load",t.captureLoad.bind(t)),r.addEventListener("DOMContentLoaded",t.captureDomContentLoaded.bind(t))),r[e]=t,t})()}function l(r){return n(function(){var o=this,e=Array.prototype.slice.call(arguments,0),n={shim:o,method:r,args:e,ts:new Date};window._rollbarShims[this.shimId()].messages.push(n)})}var i=e(2),s=0,d=e(3),c=function(r,o){return new t(r,o)},p=d.bind(null,c);t.prototype.loadFull=function(r,o,e,t,a){var l=function(){var o;if(void 0===r._rollbarDidLoad){o=new Error("rollbar.js did not load");for(var e,n,t,l,i=0;e=r._rollbarShims[i++];)for(e=e.messages||[];n=e.shift();)for(t=n.args||[],i=0;i<t.length;++i)if(l=t[i],"function"==typeof l){l(o);break}}"function"==typeof a&&a(o)},i=!1,s=o.createElement("script"),d=o.getElementsByTagName("script")[0],c=d.parentNode;s.crossOrigin="",s.src=t.rollbarJsUrl,e||(s.async=!0),s.onload=s.onreadystatechange=n(function(){if(!(i||this.readyState&&"loaded"!==this.readyState&&"complete"!==this.readyState)){s.onload=s.onreadystatechange=null;try{c.removeChild(s)}catch(r){}i=!0,l()}}),c.insertBefore(s,d)},t.prototype.wrap=function(r,o,e){try{var n;if(n="function"==typeof o?o:function(){return o||{}},"function"!=typeof r)return r;if(r._isWrap)return r;if(!r._rollbar_wrapped&&(r._rollbar_wrapped=function(){e&&"function"==typeof e&&e.apply(this,arguments);try{return r.apply(this,arguments)}catch(e){var o=e;throw"string"==typeof o&&(o=new String(o)),o._rollbarContext=n()||{},o._rollbarContext._wrappedSource=r.toString(),window._rollbarWrappedError=o,o}},r._rollbar_wrapped._isWrap=!0,r.hasOwnProperty))for(var t in r)r.hasOwnProperty(t)&&(r._rollbar_wrapped[t]=r[t]);return r._rollbar_wrapped}catch(o){return r}};for(var u="log,debug,info,warn,warning,error,critical,global,configure,handleUncaughtException,handleUnhandledRejection,captureDomContentLoaded,captureLoad".split(","),f=0;f<u.length;++f)t.prototype[u[f]]=l(u[f]);r.exports={setupShim:a,Rollbar:p}},function(r,o){"use strict";function e(r,o,e){if(r){var t;"function"==typeof o._rollbarOldOnError?t=o._rollbarOldOnError:r.onerror&&!r.onerror.belongsToShim&&(t=r.onerror,o._rollbarOldOnError=t);var a=function(){var e=Array.prototype.slice.call(arguments,0);n(r,o,t,e)};a.belongsToShim=e,r.onerror=a}}function n(r,o,e,n){r._rollbarWrappedError&&(n[4]||(n[4]=r._rollbarWrappedError),n[5]||(n[5]=r._rollbarWrappedError._rollbarContext),r._rollbarWrappedError=null),o.handleUncaughtException.apply(o,n),e&&e.apply(r,n)}function t(r,o,e){if(r){"function"==typeof r._rollbarURH&&r._rollbarURH.belongsToShim&&r.removeEventListener("unhandledrejection",r._rollbarURH);var n=function(r){var e=r.reason,n=r.promise,t=r.detail;!e&&t&&(e=t.reason,n=t.promise),o&&o.handleUnhandledRejection&&o.handleUnhandledRejection(e,n)};n.belongsToShim=e,r._rollbarURH=n,r.addEventListener("unhandledrejection",n)}}function a(r,o,e){if(r){var n,t,a="EventTarget,Window,Node,ApplicationCache,AudioTrackList,ChannelMergerNode,CryptoOperation,EventSource,FileReader,HTMLUnknownElement,IDBDatabase,IDBRequest,IDBTransaction,KeyOperation,MediaController,MessagePort,ModalWindow,Notification,SVGElementInstance,Screen,TextTrack,TextTrackCue,TextTrackList,WebSocket,WebSocketWorker,Worker,XMLHttpRequest,XMLHttpRequestEventTarget,XMLHttpRequestUpload".split(",");for(n=0;n<a.length;++n)t=a[n],r[t]&&r[t].prototype&&l(o,r[t].prototype,e)}}function l(r,o,e){if(o.hasOwnProperty&&o.hasOwnProperty("addEventListener")){for(var n=o.addEventListener;n._rollbarOldAdd&&n.belongsToShim;)n=n._rollbarOldAdd;var t=function(o,e,t){n.call(this,o,r.wrap(e),t)};t._rollbarOldAdd=n,t.belongsToShim=e,o.addEventListener=t;for(var a=o.removeEventListener;a._rollbarOldRemove&&a.belongsToShim;)a=a._rollbarOldRemove;var l=function(r,o,e){a.call(this,r,o&&o._rollbar_wrapped||o,e)};l._rollbarOldRemove=a,l.belongsToShim=e,o.removeEventListener=l}}r.exports={captureUncaughtExceptions:e,captureUnhandledRejections:t,wrapGlobals:a}},function(r,o){"use strict";function e(r,o){this.impl=r(o,this),this.options=o,n(e.prototype)}function n(r){for(var o=function(r){return function(){var o=Array.prototype.slice.call(arguments,0);if(this.impl[r])return this.impl[r].apply(this.impl,o)}},e="log,debug,info,warn,warning,error,critical,global,configure,handleUncaughtException,handleUnhandledRejection,_createItem,wrap,loadFull,shimId,captureDomContentLoaded,captureLoad".split(","),n=0;n<e.length;n++)r[e[n]]=o(e[n])}e.prototype._swapAndProcessMessages=function(r,o){this.impl=r(this.options);for(var e,n,t;e=o.shift();)n=e.method,t=e.args,this[n]&&"function"==typeof this[n]&&("captureDomContentLoaded"===n||"captureLoad"===n?this[n].apply(this,[t[0],e.ts]):this[n].apply(this,t));return this},r.exports=e},function(r,o){"use strict";r.exports=function(r){return function(o){if(!o&&!window._rollbarInitialized){r=r||{};for(var e,n,t=r.globalAlias||"Rollbar",a=window.rollbar,l=function(r){return new a(r)},i=0;e=window._rollbarShims[i++];)n||(n=e.handler),e.handler._swapAndProcessMessages(l,e.messages);window[t]=n,window._rollbarInitialized=!0}}}}]);
     }
    </script>
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-113913768-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-113913768-1');
    </script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          displayAlign: "left",
          displayIndent: "2em"
        });
    </script>
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_SVG"></script>

    

  <meta name="description" content="Arguably, we don't care about the exact cost-effectiveness estimates of
each of GiveWell's top charities. Instead, we care about their relative
values. By using distance metrics across these multidimensional outputs,
we can perform uncertainty and sensitivity analysis to answer questions
about:

-   how uncertain we are about the overall relative values of the
    charities
-   which input parameters this overall relative valuation is most
    sensitive to">

</head>
<body>
<div id="underlay">
<nav><a href="../../">Home</a></nav>
<main><article>
  <h2 id="article-title">Uncertainty and sensitivity analysis of GiveWell's top charity rankings</h2>
  
  <div class="metadata">
  <nav class="tags"><ul>
    
</ul></nav>

  
    <nav><a class="series" href="../../series/GiveWell%2520cost-effectiveness%2520analysis%2520analysis/">Series: GiveWell cost-effectiveness analysis analysis</a></nav>
  
  <span class="date">Published on <time datetime="2019-08-29">August 29, 2019</time>.</span>
  
    <br><span class="date">Last edited on <time datetime="2019-08-31">August 31, 2019</time>.</span>
  
  </div>
  
  
    <div class="abstract">
      <p>Arguably, we don’t care about the exact cost-effectiveness estimates of each of GiveWell’s top charities. Instead, we care about their relative values. By using distance metrics across these multidimensional outputs, we can perform uncertainty and sensitivity analysis to answer questions about:</p>
<ul>
<li>how uncertain we are about the overall relative values of the charities</li>
<li>which input parameters this overall relative valuation is most sensitive to</li>
</ul>
    </div>
  

  

  <p>In the <a href="../../posts/uncertainty-analysis-of-givewell-cea/">last</a> <a href="../../posts/sensitivity-analysis-of-givewell-cea/">two</a> posts, we performed uncertainty and sensitivity analyses on GiveWell’s charity cost-effectiveness estimates. Our outputs were, respectively:</p>
<ul>
<li>probability distributions describing our uncertainty about the value per dollar obtained for each charity and</li>
<li>estimates of how sensitive each charity’s cost-effectiveness is to each of its input parameters</li>
</ul>
<p>One problem with this is that we are <a href="https://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/">not supposed to take the cost-effectiveness estimates literally</a>. Arguably, the real purpose of GiveWell’s analysis is not to produce exact numbers but to assess the relative quality of each charity evaluated.</p>
<p>Another issue is that by treating each cost-effectiveness estimate as independent we underweight parameters which are shared across many models. For example, the moral weight that ought to be assigned to increasing consumption shows up in many models. If we consider all the charity-specific models together, this input seems to become more important.</p>
<h3 id="metrics-on-rankings">Metrics on rankings</h3>
<p>We can solve both of these problems by abstracting away from particular values in the cost-effectiveness analysis and looking at the overall rankings returned. That is we want to transform:</p>
<figure>
<figcaption>
GiveWell’s cost-effectiveness estimates for its top charities
</figcaption>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Charity</th>
<th style="text-align: right;">Value per $10,000 donated</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">GiveDirectly</td>
<td style="text-align: right;">38</td>
</tr>
<tr class="even">
<td style="text-align: left;">The END Fund</td>
<td style="text-align: right;">222</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Deworm the World</td>
<td style="text-align: right;">738</td>
</tr>
<tr class="even">
<td style="text-align: left;">Schistosomiasis Control Initiative</td>
<td style="text-align: right;">378</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Sightsavers</td>
<td style="text-align: right;">394</td>
</tr>
<tr class="even">
<td style="text-align: left;">Malaria Consortium</td>
<td style="text-align: right;">326</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Against Malaria Foundation</td>
<td style="text-align: right;">247</td>
</tr>
<tr class="even">
<td style="text-align: left;">Helen Keller International</td>
<td style="text-align: right;">223</td>
</tr>
</tbody>
</table>
</figure>
<p>into:</p>
<figure>
<figcaption>
Givewell’s top charities ranked from most cost-effective to least
</figcaption>
<ul>
<li>Deworm the World</li>
<li>Sightsavers</li>
<li>Schistosomiasis Control Initiative</li>
<li>Malaria Consortium</li>
<li>Against Malaria Foundation</li>
<li>Helen Keller International</li>
<li>The END Fund</li>
<li>GiveDirectly</li>
</ul>
</figure>
<p>But how do we usefully express <span class="noted">probabilities over rankings</span><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> (rather than probabilities over simple cost-effectivness numbers)? The approach we’ll follow below is to characterize a ranking produced by a run of the model by computing its distance from the reference ranking listed above (i.e. GiveWell’s current best estimate). Our output probability distribution will then express how far we expect to be from the reference ranking—how much we might learn about the ranking with more information on the inputs. For example, if the distribution is narrow and near 0, that means our uncertain input parameters mostly produce results similar to the reference ranking. If the distribution is wide and far from 0, that means our uncertain input parameters produce results that are highly uncertain and not necessarily similar to the reference ranking.</p>
<h4 id="spearmans-footrule">Spearman’s footrule</h4>
<p>What is this mysterious distance metric between rankings that enables the above approach? One such metric is called Spearman’s footrule distance. It’s defined as:</p>
<div class=".skippable">
<p><span class="math display">\[d_{fr}(u, v) = \sum_{c \in A} |\text{pos}(u,c) - \text{pos}(v, c)|\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> are rankings,</li>
<li><span class="math inline">\(c\)</span> varies over all the elements <span class="math inline">\(A\)</span> of the rankings and</li>
<li><span class="math inline">\(\text{pos}(r, x)\)</span> returns the integer position of item <span class="math inline">\(x\)</span> in ranking <span class="math inline">\(r\)</span>.</li>
</ul>
</div>
<p>In other words, the footrule distance between two rankings is the sum over all items of the (absolute) difference in positions for each item. (We also add a normalization factor so that the distance varies ranges from 0 to 1 but omit that trivia here.)</p>
<p>So the distance between A, B, C and A, B, C is 0; the (unnormalized) distance between A, B, C and C, B, A is 4; and the (unnormalized) distance between A, B, C and B, A, C is 2.</p>
<h4 id="kendalls-tau">Kendall’s tau</h4>
<p>Another common distance metric between rankings is <a href="https://en.wikipedia.org/wiki/Kendall_tau_distance">Kendall’s tau</a>. It’s defined as:</p>
<div class=".skippable">
<p><span class="math display">\[d_{tau}(u, v) = \sum_{\{i,j\} \in P} \bar{K}_{i,j}(u, v)\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> are again rankings,</li>
<li><span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are items in the set of unordered pairs <span class="math inline">\(P\)</span> of distinct elements in <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span></li>
<li><span class="math inline">\(\bar{K}_{i,j}(u, v) = 0\)</span> if <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are in the same order (concordant) in <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> and <span class="math inline">\(\bar{K}_{i,j}(u, v) = 1\)</span> otherwise (discordant)</li>
</ul>
</div>
<p>In other words, the Kendall tau distance looks at all possible pairs across items in the rankings and counts up the ones where the two rankings disagree on the ordering of these items. (There’s also a normalization factor that we’ve again omitted so that the distance ranges from 0 to 1.)</p>
<p>So the distance between A, B, C and A, B, C is 0; the (unnormalized) distance between A, B, C and C, B, A is 3; and the (unnormalized) distance between A, B, C and B, A, C is 1.</p>
<h4 id="angular-distance">Angular distance</h4>
<p>One drawback of the above metrics is that they throw away information in going from the table with cost-effectiveness estimates to a simple ranking. What would be ideal is to keep that information and find some other distance metric that still emphasizes the relationship between the various numbers rather than their precise values.</p>
<p>Angular distance is a metric which satisfies these criteria. We can regard the table of charities and cost-effectiveness values as an 8-dimensional vector. When our output produces another vector of cost-effectiveness estimates (one for each charity), we can compare this to our reference vector by finding <span class="noted">the angle between the two</span><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<!--more-->
<h3 id="results">Results</h3>
<h4 id="uncertainties">Uncertainties</h4>
<p>To recap, what we’re about to see next is the result of running our model many times with different sampled input values. In each run, we compute the cost-effectiveness estimates for each charity and compare those estimates to the reference ranking (GiveWell’s best estimate) using each of the tau, footrule and angular distance metrics. Again, the plots below are from running the analysis while pretending that we’re equally uncertain about each input parameter. To avoid this limitation, go to the <a href="https://colab.research.google.com/drive/1TCXBi7lF69Xaaygub5HGD6-Rb6qE924e#sandboxMode=true">Jupyter notebook</a> and adjust the input distributions.</p>
<figure class="natural-fig">
<img src="../../images/givewell-analysis/uncertainties-small-multiples-distances.png" title="fig:" alt="Probability distributions of value per dollar for each of GiveWell’s top charity and probability distributions for the distance between model results and the reference results" />
<figcaption>
Probability distributions of value per dollar for each of GiveWell’s top charity and probability distributions for the distance between model results and the reference results
</figcaption>
</figure>
<p>We see that our input uncertainty does matter even for these highest level results—there are some input values which cause the ordering of best charities to change. If the gaps between the cost-effectiveness estimates had been very large or our input uncertainty had been very small, we would have expected essentially all of the probability mass to be concentrated at 0 because no change in inputs would have been enough to meaningfully change the relative cost-effectiveness of the charities.</p>
<h4 id="visual-sensitivity-analysis">Visual sensitivity analysis</h4>
<p>We can now repeat our visual sensitivity analysis but using our distance metrics from the reference as our outcome of interest instead of individual cost-effectiveness estimates. What these plots show is how sensitive the relative cost-effectiveness of the different charities is to each of the input parameters used in any of the cost-effectiveness models (so, yes, there are a lot of parameters/plots). We have three big plots, one for each distance metric—footrule, tau and angle. In each plot, there’s a subplot corresponding to each input factor used anywhere in the GiveWell’s cost-effectiveness analysis.</p>
<figure class="natural-fig">
<figcaption>
Scatter plots showing sensitivity of the footrule distance with respect to each input parameter
</figcaption>
<img src="../../images/givewell-analysis/regressions-max-footrule-footrule.png" title="fig:" alt="Scatter plots showing sensitivity of the footrule distance with respect to each input parameter" />
</figure>
<figure class="natural-fig">
<figcaption>
Scatter plots showing sensitivity of the tau distance with respect to each input parameter
</figcaption>
<img src="../../images/givewell-analysis/regressions-max-tau-tau.png" title="fig:" alt="Scatter plots showing sensitivity of the tau distance with respect to each input parameter" />
</figure>
<figure class="natural-fig">
<figcaption>
Scatter plots showing sensitivity of the angular distance with respect to each input parameter
</figcaption>
<img src="../../images/givewell-analysis/regressions-max-angle-angle.png" title="fig:" alt="Scatter plots showing sensitivity of the angular distance with respect to each input parameter" />
</figure>
<p>(The banding in the tau and footrule plots is just an artifact of those distance metrics returning integers (before normalization) rather than reals.)</p>
<p>These results might be a bit surprising at first. Why are there so many charity-specific factors with apparently high sensitivity indicators? Shouldn’t input parameters which affect <em>all</em> models have the biggest influence on the overall result? Also, why do so few of the factors that showed up as most influential in <a href="../../posts/sensitivity-analysis-of-givewell-cea/">the charity-specific sensitivity analyses from last time</a> make it to the top?</p>
<p>However, after reflecting for a bit, this makes sense. Because we’re interested in the <em>relative</em> performance of the charities, any factor which affects them all equally is of little importance here. Instead, we want factors that have a strong influence on only a few charities. When we go back to the earlier charity-by-charity sensitivity analysis, we see that many of the input parameters we identified as most influential where shared across charities (especially across the deworming charities). Non-shared factors that made it to the top of the charity-by-charity lists—like the relative risk of all-cause mortality for young children in <abbr title="Vitamin A supplementation">VAS</abbr> programs—show up somewhat high here too.</p>
<p>But it’s hard to eyeball the sensitivity when there are so many factors and most are of small effect. So let’s quickly move on to the delta analysis.</p>
<h4 id="delta-moment-independent-sensitivity-analysis">Delta moment-independent sensitivity analysis</h4>
<p>Again, we’ll have three big plots, one for each distance metric—footrule, tau and angle. In each plot, there’s an estimate of the delta moment-independent sensitivity for each input factor used anywhere in the GiveWell’s cost-effectiveness analysis (and an indication of how confident that sensitivity estimate is).</p>
<figure class="natural-fig">
<figcaption>
Delta sensitivities for each input parameter in footrule distance analysis
</figcaption>
<img src="../../images/givewell-analysis/sensitivity-max-footrule-delta.png" title="fig:" alt="Delta sensitivities for each input parameter in footrule distance analysis" />
</figure>
<figure class="natural-fig">
<figcaption>
Delta sensitivities for each input parameter in the tau distance analysis
</figcaption>
<img src="../../images/givewell-analysis/sensitivity-max-tau-delta.png" title="fig:" alt="Delta sensitivities for each input parameter in the tau distance analysis" />
</figure>
<figure class="natural-fig">
<figcaption>
Delta sensitivities for each input parameter in angular distance analysis
</figcaption>
<img src="../../images/givewell-analysis/sensitivity-max-angle-delta.png" title="fig:" alt="Delta sensitivities for each input parameter in angular distance analysis" />
</figure>
<p>So these delta sensitivities corroborate the suspicion that arose during the visual sensitivity analysis—charity-specific input parameters have the highest sensitivity indicators.</p>
<p>The other noteworthy result is which charity-specific factors are the most influential depends somewhat on which distance metric we use. The two rank-based metrics—tau and footrule distance—both suggest that the final charity ranking (given these inputs) is most sensitive to the worm intensity adjustment and cost per capita per annum of Sightsavers and the END Fund. These input parameters are a bit further down (though still fairly high) in the list according to the angular distance metric.</p>
<h5 id="needs-more-meta">Needs more meta</h5>
<p>It would be nice to check that our distance metrics don’t produce totally contradictory results. How can we accomplish this? Well, the plots above already order the input factors according to their sensitivity indicators… That means we have rankings of the sensitivities of the input factors and we can compare the rankings using Kendall’s tau and Spearman’s footrule distance. If that sounds confusing hopefully the table clears things up:</p>
<figure>
<figcaption>
Using Kendall’s tau and Spearman’s footrule distance to assess the similarity of sensitivity rankings generated under different distance metrics
</figcaption>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Delta sensitivity rankings compared</th>
<th style="text-align: right;">Tau distance</th>
<th style="text-align: right;">Footrule distance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Tau and footrule</td>
<td style="text-align: right;">0.358</td>
<td style="text-align: right;">0.469</td>
</tr>
<tr class="even">
<td style="text-align: left;">Tau and angle</td>
<td style="text-align: right;">0.365</td>
<td style="text-align: right;">0.516</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Angle and footrule</td>
<td style="text-align: right;">0.430</td>
<td style="text-align: right;">0.596</td>
</tr>
</tbody>
</table>
</figure>
<p>So it looks like the three rankings have middling agreement. Sensitivities according to tau and footrule agree the most while sensitivities according to angle and footrule agree the least. The disagreement probably also reflects random noise since the confidence intervals for many of the variables’ sensitivity indicators overlap. We could presumably shrink these confidence intervals and reduce the noise by increasing the number of samples used during our analysis.</p>
<p>To the extent that the disagreement isn’t just noise, it’s not entirely surprising—part of the point of using different distance metrics is to capture different notions of distance, each of which might be more or less suitable for a given purpose. But the divergence does mean that we’ll need to carefully pick which metric to pay attention to depending on the precise questions we’re trying to answer. For example, if we just want to pick the single top charity and donate all our money to that, factors with high sensitivity indicators according to footrule distance might be the most important to pin down. On the other hand, if we want to distribute our money in proportion to each charity’s estimated cost-effectiveness, angular distance is perhaps a better metric to guide our investigations.</p>
<h3 id="conclusion">Conclusion</h3>
<p>We started with a couple of problems with our previous analysis: we were taking cost-effectiveness estimates literally and looking at them independently instead of as parts of a cohesive analysis. We addressed these problems by redoing our analysis while looking at distance from the current best cost-effectiveness estimates. We found that our input uncertainty is consequential even when looking only at the relative cost-effectiveness of the charities. We also found that input parameters which are important but unique to a particular charity often affect the final relative cost-effectiveness substantially.</p>
<p>Finally, we have the same caveat as last time: these results still reflect my fairly arbitrary (but scrupulously neutral) decision to pretend that we equally uncertain about each input parameter. To remedy this flaw and get results which are actually meaningful, head over to the <a href="https://colab.research.google.com/drive/1TCXBi7lF69Xaaygub5HGD6-Rb6qE924e#sandboxMode=true">Jupyter notebook</a> and tweak the input distributions.</p>
<h3 id="appendix">Appendix</h3>
<p>We can also look at the sensitivities based on the Sobol method again.</p>
<p>The variable order in each plot is from the input parameter with the highest <span class="math inline">\(\delta_i\)</span> sensitivity to the input parameter with the lowest <span class="math inline">\(\delta_i\)</span> sensitivity. That makes it straightforward to compare the ordering of sensitivities according to the delta moment-independent method and according to the Sobol method. We see that there is broad—but not perfect—agreement between the different methods.</p>
<figure class="natural-fig">
<figcaption>
Sobol sensitivities for each input parameter in footrule distance analysis
</figcaption>
<img src="../../images/givewell-analysis/sensitivity-max-footrule-s1.png" title="fig:" alt="Sobol sensitivities for each input parameter in footrule distance analysis" />
</figure>
<figure class="natural-fig">
<figcaption>
Sobol sensitivities for each input parameter in tau distance analysis
</figcaption>
<img src="../../images/givewell-analysis/sensitivity-max-tau-s1.png" title="fig:" alt="Sobol sensitivities for each input parameter in tau distance analysis" />
</figure>
<figure class="natural-fig">
<figcaption>
Sobol sensitivities for each input parameter in angular distance analysis
</figcaption>
<img src="../../images/givewell-analysis/sensitivity-max-angle-s1.png" title="fig:" alt="Sobol sensitivities for each input parameter in angular distance analysis" />
</figure>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>If we just look at the probability for each possible ranking independently, we’ll be overwhelmed by the number of permutations and it will be hard to find any useful structure in our results.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩</a></p></li>
<li id="fn2" role="doc-endnote"><p>The angle between the vectors is a better metric here than the distance between the vectors’ endpoints because we’re interested in the relative cost-effectiveness of the charities and how those change. If our results show that each charity is twice as effective as in the reference vector, our metric should return a distance of 0 because nothing has changed in the relative cost-effectiveness of each charity.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩</a></p></li>
</ol>
</section>
</article></main>

</div>




<script defer type="text/javascript" src="../../js/custom-elements.js"></script>

<script defer type="text/javascript" src="../../js/vendors~arg-map~custom-elements~util-egal.js"></script>



  </body>
</html>
